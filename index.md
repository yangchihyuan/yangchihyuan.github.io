---
layout: home
title: "Home"
---

<h3 class="fw-bold">Current Projects</h3>

#### robot nurse helper
<iframe width="560" height="315" src="https://www.youtube.com/embed/nSkMEZk-ln4?si=W169XRhEbcba3F6K" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/gLJKk88el-U?si=Fk2D7acL8COeuUGV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

This is a project for developing a companion robot to help nurses in a pediatric ward. There is a shortage of nurses in Taiwan, due to the change of population structure caused by sub-replacement fertility. Our goal is to enable a social to take some nurse tasks to reduce the working load of human nurses. Our first task is to monitor pediatric patients' emotion and their levels of painfulness.
Introduction slides [(Link)](https://www.dropbox.com/scl/fi/zmytc7e8zdvyo8svnumpc/Zenbo-Nurse-Helper.pptx?rlkey=yfq7dhkctzgfsa6h7bca49oe5&dl=0)
Code is available on my GitHub webpage [(Link)](https://github.com/yangchihyuan/ZenboNurseHelper).

#### Depth estimation in an eye ball

<img src="http://yangchihyuan.github.io/assets/img/microsurgery.jpg" height="250" alt="microsurgery">
<img src="http://yangchihyuan.github.io/assets/img/Eyeball.jpg" height="250" alt="Eyeball">
<img src="http://yangchihyuan.github.io/assets/img/007_MH_RD_3D_merged.jpg" height="250" alt="Stereo images">

For intraocular microsurgery, robotic assistance is a cutting-edge research field because it is promising for expanding human capabilities and improving the safety and efficiency of the intricate surgery process. Because depth is critical for intraocular microsurgery, in this project we want to estimate the depth of the medical devices in an eye ball. We want to develop a method which can warm the operator if the device is too close to the retina.

#### Zero-shot skeleton-based action recognition
<img src="http://yangchihyuan.github.io/publications/ECCV_2024_SA_DVAE.jpg" height="300" alt="SA_DVAE">

Current research results have been published in ECCV'24 and the paper is available in my publication webpage [(Link)](https://yangchihyuan.github.io/publications). However, there are some puzzles unsolved yet. For example, why does this method work poorly on some action datasets? How will the quality of text affect the method's performance? If we refine the text description of the NTU RGB+D dataset, what will happen? Is the skeleton data of the NTU RGB+D dataset so noisy that the proposed method works? Given a high-quality skeleton action dataset, will the proposed method still work?
To answer those questions, further research is required to be carried out.
